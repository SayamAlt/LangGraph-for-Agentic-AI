{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76029759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25a2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant with memory capabilities.\n",
    "If user-specific memory is available, use it to personalize \n",
    "your responses based on what you know about the user.\n",
    "\n",
    "Your goal is to provide relevant, friendly, and tailored \n",
    "assistance that reflects the user’s preferences, context, and past interactions.\n",
    "\n",
    "If the user’s name or relevant personal context is available, always personalize your responses by:\n",
    "    – Always Address the user by name (e.g., \"Sure, Nitish...\") when appropriate\n",
    "    – Referencing known projects, tools, or preferences (e.g., \"your MCP server python based project\")\n",
    "    – Adjusting the tone to feel friendly, natural, and directly aimed at the user\n",
    "\n",
    "Avoid generic phrasing when personalization is possible.\n",
    "\n",
    "Use personalization especially in:\n",
    "    – Greetings and transitions\n",
    "    – Help or guidance tailored to tools and frameworks the user uses\n",
    "    – Follow-up messages that continue from past context\n",
    "\n",
    "Always ensure that personalization is based only on known user details and not assumed.\n",
    "\n",
    "In the end suggest 3 relevant further questions based on the current response and user profile\n",
    "\n",
    "The user’s memory (which may be empty) is provided as: {user_details_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce7fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory extraction LLM\n",
    "memory_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3556a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryItem(BaseModel):\n",
    "    content: str = Field(description=\"Atomic user memory in a short sentence\")\n",
    "    is_new: bool = Field(description=\"TRUE if the memory is new to the user, FALSE if duplicated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee688fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryDecision(BaseModel):\n",
    "    create_memory: bool = Field(description=\"Whether to create a new memory based on user's messages\")\n",
    "    memories: List[MemoryItem] = Field(default_factory=list,description=\"List of atomic memories to be created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c51d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_extractor = memory_llm.with_structured_output(MemoryDecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b758e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_PROMPT = \"\"\"You are responsible for updating and maintaining accurate user memory.\n",
    "\n",
    "CURRENT USER DETAILS (existing memories):\n",
    "{user_details_content}\n",
    "\n",
    "TASK:\n",
    "- Review the user's latest message.\n",
    "- Extract user-specific info worth storing long-term (identity, stable preferences, ongoing projects/goals).\n",
    "- For each extracted item, set is_new=true ONLY if it adds NEW information compared to CURRENT USER DETAILS.\n",
    "- If it is basically the same meaning as something already present, set is_new=false.\n",
    "- Keep each memory as a short atomic sentence.\n",
    "- No speculation; only facts stated by the user.\n",
    "- If there is nothing memory-worthy, return should_write=false and an empty list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebcc3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "   user_id = config['configurable']['user_id']\n",
    "   \n",
    "   namespace = ('user', user_id, 'responses')\n",
    "   \n",
    "   # Retrieve existing user memories\n",
    "   items = store.search(namespace)\n",
    "   existing_memories = \"\\n\".join(item.value['data'] for item in items) if items else \"\"\n",
    "   \n",
    "   # Get latest user message\n",
    "   last_message = state['messages'][-1].content\n",
    "   \n",
    "   decision: MemoryDecision = memory_extractor.invoke(\n",
    "       [\n",
    "           SystemMessage(content=MEMORY_PROMPT.format(user_details_content=existing_memories)),\n",
    "           {\"role\": \"user\", \"content\": f\"User Message:\\n{last_message}\"}\n",
    "       ]\n",
    "   )\n",
    "   \n",
    "   if decision.create_memory:\n",
    "       for memory in decision.memories:\n",
    "           if memory.is_new:\n",
    "               store.put(namespace, str(uuid.uuid4()), {\"data\": memory.content})\n",
    "   return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2772f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bcc86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n",
    "    user_id = config['configurable']['user_id']\n",
    "    \n",
    "    namespace = ('user', user_id, 'responses')\n",
    "    \n",
    "    # Retrieve existing user memories\n",
    "    items = store.search(namespace)\n",
    "    existing_memories = \"\\n\".join(item.value['data'] for item in items) if items else \"\"\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=SYSTEM_PROMPT_TEMPLATE.format(\n",
    "            user_details_content=existing_memories or \"empty\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = chat_llm.invoke([system_message] + state['messages'])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22cf2a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x11080e590>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a state graph\n",
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph.add_node(\"remember\", remember_node)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "\n",
    "# Add edges to the graph\n",
    "graph.add_edge(START, \"remember\")\n",
    "graph.add_edge(\"remember\", \"chat\")\n",
    "graph.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e91338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Sayam! To get the optimal weights and biases for a given dataset using gradient descent, you'll typically follow these steps:\n",
      "\n",
      "1. **Initialize Weights and Biases**: Start by initializing the weights (often randomly) and biases (often to zero).\n",
      "\n",
      "2. **Define the Model**: Choose a model architecture. For example, if you're working on a simple linear regression, your model can be represented as:\n",
      "   \\[\n",
      "   y = wx + b\n",
      "   \\]\n",
      "   where \\(w\\) is the weight, \\(b\\) is the bias, and \\(x\\) is the input feature.\n",
      "\n",
      "3. **Choose a Loss Function**: The loss function measures how well your model predicts the actual values in your dataset. Common loss functions include Mean Squared Error (MSE) for regression or Cross-Entropy for classification.\n",
      "\n",
      "4. **Compute the Gradient**: Calculate the gradient of the loss function with respect to the weights and biases. This indicates the direction in which we need to adjust our weights and biases to minimize the loss. If you're using MSE, your gradients will look something like this:\n",
      "   \\[\n",
      "   \\text{Gradient w.r.t. } w = \\frac{1}{N} \\sum (y_{\\text{pred}} - y) \\cdot x\n",
      "   \\]\n",
      "   \\[\n",
      "   \\text{Gradient w.r.t. } b = \\frac{1}{N} \\sum (y_{\\text{pred}} - y)\n",
      "   \\]\n",
      "\n",
      "5. **Update Weights and Biases**: Adjust the weights and biases using the gradients calculated in the previous step. The update rule can be expressed as:\n",
      "   \\[\n",
      "   w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
      "   \\]\n",
      "   \\[\n",
      "   b = b - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
      "   \\]\n",
      "   where \\(\\eta\\) is the learning rate, which controls how large a step you take towards the minimum.\n",
      "\n",
      "6. **Iterate**: Repeat steps 4 and 5 for a number of iterations (or until your loss function converges). This is where the \"descent\" in gradient descent comes from as you move closer to the minimum loss.\n",
      "\n",
      "7. **Monitor Performance**: It can be helpful to track the loss over iterations to ensure that you are converging.\n",
      "\n",
      "Let me know if you’d like me to expand on any of these steps, or if you’re working with a specific type of dataset or model in mind!\n",
      "\n",
      "Here are a few questions to consider:\n",
      "1. Are you currently working on a specific machine learning project?\n",
      "2. What model are you interested in applying gradient descent to?\n",
      "3. Would you like to know about any specific libraries or tools to implement this in Python?\n",
      "\n",
      "--- Stored Memories (from Postgres) ---\n",
      "Sayam loves learning about data science, AI, and ML.\n",
      "User's name is Sayam.\n"
     ]
    }
   ],
   "source": [
    "# Use PostgresStore as persistent LTM\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    store.setup()\n",
    "    workflow = graph.compile(store=store)\n",
    "    \n",
    "    config = {\"configurable\": {\"user_id\": \"u1\"}}\n",
    "    \n",
    "    # Invoke the graph\n",
    "    workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Sayam\"}]}, config)\n",
    "    workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I love learning about data science, AI, and ML.\"}]}, config)\n",
    "    \n",
    "    output = workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Can you explain how to get the optimal weights and biases for a given dataset using gradient descent?\"}]}, config=config)\n",
    "    \n",
    "    print(output['messages'][-1].content)\n",
    "    \n",
    "    print(\"\\n--- Stored Memories (from Postgres) ---\")\n",
    "    for item in store.search((\"user\", \"u1\", \"responses\")):\n",
    "        print(item.value['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85011aa",
   "metadata": {},
   "source": [
    "## Check persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3165b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sayam loves learning about data science, AI, and ML.\n",
      "User's name is Sayam.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with PostgresStore.from_conn_string(DB_URI) as store:\n",
    "    namespace = ('user', 'u1', 'responses')\n",
    "    items = store.search(namespace)\n",
    "\n",
    "for item in items:\n",
    "    print(item.value['data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
