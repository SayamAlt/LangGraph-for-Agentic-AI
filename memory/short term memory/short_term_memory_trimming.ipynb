{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1181ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages.utils import trim_messages, count_tokens_approximately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0e67f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49475ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d60dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b5374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    # Trim conversation history to return last N messages that don't exceed MAX_TOKENS\n",
    "    messages = trim_messages(\n",
    "        state['messages'],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    print('Current Token Count:', count_tokens_approximately(messages=messages))\n",
    "    \n",
    "    for message in messages:\n",
    "        print(message.content)\n",
    "        \n",
    "    response = llm.invoke(messages)\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c87a12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10e06a350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the state graph\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node('call_model', call_model)\n",
    "graph.add_edge(START, 'call_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54e5f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x10e06ad10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "workflow = graph.compile(checkpointer=checkpointer)\n",
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a5731af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Count: 10\n",
      "Hi, my name is Sayam.\n",
      "Hello Sayam, nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {'configurable': {'thread_id': 'thread-1'}}\n",
    "\n",
    "result = workflow.invoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            ('user', 'Hi, my name is Sayam.')\n",
    "        ],\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcbccaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Count: 56\n",
      "Hi, my name is Sayam.\n",
      "Hello Sayam, nice to meet you. How can I assist you today?\n",
      "What is my name?\n",
      "Your name is Sayam.\n",
      "What is my name?\n",
      "Your name is Sayam.\n"
     ]
    }
   ],
   "source": [
    "result = workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]}, config=config)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16dbfeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Count: 85\n",
      "Hi, my name is Sayam.\n",
      "Hello Sayam, nice to meet you. How can I assist you today?\n",
      "What is my name?\n",
      "Your name is Sayam.\n",
      "What is my name?\n",
      "Your name is Sayam.\n",
      "I am learning short-term memory implementation in LangGraph.\n",
      "That's great to hear! LangGraph is a powerful tool for learning about language and memory concepts. If you have any questions or need help with anything related to short-term memory implementation in LangGraph, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "result = workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I am learning short-term memory implementation in LangGraph.\"}]}, config=config)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe28ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Count: 146\n",
      "What is my name?\n",
      "Your name is Sayam.\n",
      "What is my name?\n",
      "Your name is Sayam.\n",
      "I am learning short-term memory implementation in LangGraph.\n",
      "That's great to hear! LangGraph is a powerful tool for learning about language and memory concepts. If you have any questions or need help with anything related to short-term memory implementation in LangGraph, feel free to ask.\n",
      "Explain how to get the optimal weights and biases for a given dataset using gradient descent.\n",
      "Gradient descent is an iterative optimization algorithm used to find the optimal weights and biases for a neural network model given a dataset. Here's a general overview of the steps involved in using gradient descent to update the weights and biases:\n",
      "\n",
      "1. Initialization: Initialize the weights and biases of the neural network with random values.\n",
      "\n",
      "2. Forward Propagation: Perform forward propagation to compute the output of the neural network for a given input.\n",
      "\n",
      "3. Calculate Loss: Compute the loss or error between the predicted output and the actual output using a loss function such as Mean Squared Error or Cross-Entropy Loss.\n",
      "\n",
      "4. Backpropagation: Calculate the gradient of the loss function with respect to the weights and biases of the neural network using backpropagation. This involves applying the chain rule to compute the partial derivatives of the loss function with respect to each weight and bias in the network.\n",
      "\n",
      "5. Update Weights and Biases: Update the weights and biases of the network based on the computed gradients. This is done by moving in the opposite direction of the gradient with a certain step size known as the learning rate. The update rule is given by:\n",
      "   new_weight = old_weight - learning_rate * gradient_weight\n",
      "   new_bias = old_bias - learning_rate * gradient_bias\n",
      "\n",
      "6. Repeat: Repeat steps 2-5 for multiple iterations or epochs until the loss function converges to a minimum value.\n",
      "\n",
      "By following the above steps, gradient descent helps to adjust the weights and biases of the neural network towards the optimal values that minimize the error on the training dataset, leading to better model performance on unseen data.\n"
     ]
    }
   ],
   "source": [
    "result = workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain how to get the optimal weights and biases for a given dataset using gradient descent.\"}]}, config=config)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc63ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Token Count: 8\n",
      "What is my name?\n",
      "I'm sorry, I do not know your name as we are communicating through text.\n"
     ]
    }
   ],
   "source": [
    "result = workflow.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is my name?\"}]}, config=config)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc336275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is Sayam.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Hello Sayam, nice to meet you. How can I assist you today?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "What is my name?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Your name is Sayam.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "What is my name?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Your name is Sayam.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "I am learning short-term memory implementation in LangGraph.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "That's great to hear! LangGraph is a powerful tool for learning about language and memory concepts. If you have any questions or need help with anything related to short-term memory implementation in LangGraph, feel free to ask.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Explain how to get the optimal weights and biases for a given dataset using gradient descent.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Gradient descent is an iterative optimization algorithm used to find the optimal weights and biases for a neural network model given a dataset. Here's a general overview of the steps involved in using gradient descent to update the weights and biases:\n",
      "\n",
      "1. Initialization: Initialize the weights and biases of the neural network with random values.\n",
      "\n",
      "2. Forward Propagation: Perform forward propagation to compute the output of the neural network for a given input.\n",
      "\n",
      "3. Calculate Loss: Compute the loss or error between the predicted output and the actual output using a loss function such as Mean Squared Error or Cross-Entropy Loss.\n",
      "\n",
      "4. Backpropagation: Calculate the gradient of the loss function with respect to the weights and biases of the neural network using backpropagation. This involves applying the chain rule to compute the partial derivatives of the loss function with respect to each weight and bias in the network.\n",
      "\n",
      "5. Update Weights and Biases: Update the weights and biases of the network based on the computed gradients. This is done by moving in the opposite direction of the gradient with a certain step size known as the learning rate. The update rule is given by:\n",
      "   new_weight = old_weight - learning_rate * gradient_weight\n",
      "   new_bias = old_bias - learning_rate * gradient_bias\n",
      "\n",
      "6. Repeat: Repeat steps 2-5 for multiple iterations or epochs until the loss function converges to a minimum value.\n",
      "\n",
      "By following the above steps, gradient descent helps to adjust the weights and biases of the neural network towards the optimal values that minimize the error on the training dataset, leading to better model performance on unseen data.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "What is my name?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "I'm sorry, I do not know your name as we are communicating through text.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in workflow.get_state({\"configurable\": {\"thread_id\": \"thread-1\"}}).values['messages']:\n",
    "    print(item.content)\n",
    "    print('---'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
